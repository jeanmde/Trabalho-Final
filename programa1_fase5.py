# -*- coding: utf-8 -*-
"""programa1_fase5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nv5N8CMGS9PkDfE-5aVVcd76bM-nUA8i

#1. Instalação e importação de bibliotecas
"""

import pandas as pd
import numpy as np
from datetime import datetime
import re
import unicodedata
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import gc # Para o coletor de lixo

# --- Configurações Iniciais e Download de Recursos NLTK ---
print("--- Configurações Iniciais e Download de Recursos NLTK ---")

# Garante download do NLTK punkt (tokenizador de frases)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Baixando recurso 'punkt' do NLTK...")
    nltk.download('punkt', quiet=True)
    print("Recurso 'punkt' baixado com sucesso.")

# Garante download do NLTK punkt_tab (tabelas para tokenização específica, ex: português)
try:
    nltk.data.find('tokenizers/punkt_tab/portuguese.pickle')
except LookupError:
    print("Baixando recurso 'punkt_tab' do NLTK (para português)...")
    nltk.download('punkt_tab', quiet=True)
    print("Recurso 'punkt_tab' baixado com sucesso.")

# Garante download das stopwords (palavras comuns para remoção)
try:
    _ = stopwords.words("portuguese")
except LookupError:
    print("Baixando recurso 'stopwords' do NLTK...")
    nltk.download("stopwords", quiet=True)
    print("Recurso 'stopwords' baixado com sucesso.")

stop_words = set(stopwords.words("portuguese"))
print("Recursos NLTK configurados com sucesso.")

# --- Funções Auxiliares (Definidas Globalmente) ---
print("\n--- Funções Auxiliares ---")

def standardize_text(text):
    """
    Padroniza o texto: converte para minúsculas, remove acentos e caracteres especiais,
    e espaços extras.
    """
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8", "ignore")
    text = re.sub(r'[^a-z0-9\s]', '', text) # Remove tudo que não for letra, número ou espaço
    return text.strip()

def clean_and_tokenize(text, min_word_len=5):
    """
    Limpa e tokeniza o texto, removendo stopwords e palavras com menos que 'min_word_len' caracteres.
    """
    text = standardize_text(text)
    if not text:
        return []
    tokens = word_tokenize(text, language='portuguese')
    # Filtra stopwords e palavras com tamanho mínimo
    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_word_len]
    return tokens

def extract_experience_years(cv_text):
    """
    Extrai o total de anos de experiência de um texto de CV, analisando datas de início e fim.
    """
    if not isinstance(cv_text, str):
        return 0
    # Expressão regular para encontrar padrões de data como DD/MM/AAAA a DD/MM/AAAA ou DD/MM/AAAA a atual
    matches = re.findall(r'\d{2}/\d{2}/\d{4}\s*(?:[áa])\s*(?:atual|\d{2}/\d{2}/\d{4})', cv_text, re.IGNORECASE)
    total_experience_days = 0
    for match in matches:
        parts = re.split(r'\s*[áa]\s*', match)
        start_date_str = parts[0].strip()
        end_date_str = parts[1].strip()
        try:
            start_date = datetime.strptime(start_date_str, '%d/%m/%Y')
            end_date = datetime.now() if end_date_str.lower() == 'atual' else datetime.strptime(end_date_str, '%d/%m/%Y')
            duration = end_date - start_date
            total_experience_days += duration.days
        except ValueError:
            continue
    return total_experience_days / 365.25

def map_nivel_profissional_to_experience(nivel):
    """
    Mapeia o nível profissional para um valor numérico representativo de anos de experiência.
    """
    nivel_map = {
        'estagiário': 0, 'junior': 1, 'pleno': 3, 'senior': 5,
        'especialista': 8, 'gerente': 10, 'diretor': 12
    }
    return nivel_map.get(standardize_text(nivel), 0)

def get_nested_value(row, path, default=None):
    """
    Acessa um valor em uma estrutura aninhada (dicionário ou Series) de forma segura.
    Ex: get_nested_value(row, "perfil_vaga.principais_atividades")
    """
    keys = path.split(".")
    value = row
    for i, key in enumerate(keys):
        # Verifica se é um dicionário e a chave existe, ou se é uma Series e a chave existe no índice
        if isinstance(value, dict) and key in value:
            value = value[key]
        elif isinstance(value, pd.Series) and key in value.index:
            value = value[key]
        else:
            return default # Retorna o padrão se o caminho não for encontrado
    return value

def calculate_age(dob_str):
    """
    Calcula a idade a partir da data de nascimento, truncando para o número inteiro.
    (Ex: 51.99 anos se torna 51 anos).
    """
    try:
        if isinstance(dob_str, str):
            if dob_str == "0000-00-00": # Lida com datas nulas/inválidas
                return np.nan
            if re.match(r'\d{2}-\d{2}-\d{4}', dob_str): # Formato DD-MM-AAAA
                dob = datetime.strptime(dob_str, "%d-%m-%Y")
            elif re.match(r'\d{4}-\d{2}-\d{2}', dob_str): # Formato YYYY-MM-DD
                dob = datetime.strptime(dob_str, "%Y-%m-%d")
            else:
                return np.nan # Formato não reconhecido
        else:
            return np.nan # Não é string

        # Calcula a idade em anos e TRUNCA para o inteiro
        return int((datetime.now() - dob).days / 365.25)
    except (ValueError, TypeError):
        return np.nan # Lida com erros de conversão

def check_age_match(age, faixa_etaria_str):
    """
    Verifica se a idade do candidato se encaixa na faixa etária da vaga.
    """
    if pd.isna(age) or not isinstance(faixa_etaria_str, str):
        return 0 # Não há match se idade ou faixa etária forem inválidas

    faixa_etaria_str_standardized = standardize_text(faixa_etaria_str)
    numbers = re.findall(r'\d+', faixa_etaria_str_standardized) # Extrai números da string

    if len(numbers) >= 2:
        try:
            min_age = int(numbers[0])
            max_age = int(numbers[1])
            return int(age >= min_age and age <= max_age) # Retorna 1 se dentro da faixa, 0 caso contrário
        except ValueError:
            return 0 # Erro ao converter números
    return 0 # Faixa etária não formatada corretamente

def check_language_match(candidato_level, vaga_level):
    """
    Verifica se o nível de idioma do candidato atende ou excede o da vaga.
    """
    candidato_level = standardize_text(candidato_level)
    vaga_level = standardize_text(vaga_level)

    levels = {"nenhum": 0, "basico": 1, "intermediario": 2, "avancado": 3, "fluente": 4}

    cand_val = levels.get(candidato_level, 0)
    vaga_val = levels.get(vaga_level, 0)

    if vaga_val == 0: # Se a vaga não exige idioma, qualquer nível serve
        return 1

    return int(cand_val >= vaga_val) # Retorna 1 se o nível do candidato for suficiente

def count_certifications(cert_str):
    """Conta o número de certificações em uma string de certificações separadas por vírgula."""
    if not isinstance(cert_str, str) or not cert_str.strip():
        return 0
    certs = [standardize_text(c) for c in cert_str.split(",") if standardize_text(c)]
    return len(certs)

def extract_candidate_location(row, location_type):
    """
    Extrai informações de localização (país, estado, cidade) do candidato
    a partir de diferentes campos no DataFrame.
    """
    location_str = get_nested_value(row, "infos_basicas.local", "")
    if isinstance(location_str, str) and location_str.strip():
        standardized_parts = [p for p in re.split(r'[\s,\-]+', standardize_text(location_str)) if p]

        if location_type == 'pais':
            if 'brasil' in standardized_parts: return 'brasil'
            return 'brasil' # Padrão para Brasil se não especificado ou se for um local brasileiro genérico

        elif location_type == 'estado':
            if 'rio' in standardized_parts and 'de' in standardized_parts and 'janeiro' in standardized_parts:
                return 'rj' # Padronizando para sigla
            if 'sao' in standardized_parts and 'paulo' in standardized_parts:
                return 'sp' # Padronizando para sigla
            if len(standardized_parts[-1]) == 2 and standardized_parts[-1] in ["sp", "rj", "mg", "ba", "rs", "pr", "sc", "df", "go", "es", "ce", "pe", "pa", "am", "al", "ap", "pb", "pi", "rn", "se", "to", "ro", "rr", "ac", "ma", "ms", "mt"]:
                return standardized_parts[-1]
            if len(standardized_parts) > 1 and standardized_parts[1] not in ['de', 'da', 'do']:
                return standardized_parts[1]
            return ""

        elif location_type == 'cidade':
            if standardized_parts:
                if 'rio' in standardized_parts and 'de' in standardized_parts and 'janeiro' in standardized_parts:
                    return 'rio de janeiro'
                if 'sao' in standardized_parts and 'paulo' in standardized_parts:
                    return 'sao paulo'
                return standardized_parts[0] # Assume o primeiro termo como cidade
            return ""

    address_str = standardize_text(get_nested_value(row, "informacoes_pessoais.endereco", ""))
    if address_str:
        if location_type == 'cidade':
            return address_str
        elif location_type == 'estado':
            if 'sao paulo' in address_str: return 'sp'
            if 'rio de janeiro' in address_str: return 'rj'
            return ""
        elif location_type == 'pais':
            if 'brasil' in address_str: return 'brasil'
            return 'brasil' # Assume Brasil se endereço existir

    return ""
print("Funções auxiliares definidas e prontas para uso.")

"""#2. Carregamento dos dados


"""

# --- 2. Carregamento dos dados ---
print("\n### 2. Carregamento dos dados")
print("Por favor, selecione os arquivos 'applicants.json', 'prospects.json' e 'vagas.json' para upload.")
# uploaded = files.upload() # Descomente esta linha se estiver executando no Google Colab pela primeira vez na sessão.

# Para fins de demonstração e execução local, ou para simular o ambiente do Colab após o upload,
# os arquivos JSON precisam estar acessíveis (na mesma pasta ou com o caminho completo).
# Se estiver no Colab e já fez o upload, as linhas abaixo lerão os arquivos do ambiente de execução.

try:
    df_applicants = pd.read_json("applicants.json", orient="index")
    df_prospects = pd.read_json("prospects.json", orient="index")
    df_vagas = pd.read_json("vagas.json", orient="index")
    print("Arquivos JSON carregados com sucesso.")
except FileNotFoundError as e:
    print(f"Erro ao carregar arquivo: {e}. Certifique-se de que os arquivos foram enviados ou estão no caminho correto.")
    # Adicione aqui um sys.exit() ou levante uma exceção se a ausência dos arquivos for fatal.
    # Por agora, apenas continue para mostrar a estrutura, mas o DataFrame estaria vazio.
    df_applicants = pd.DataFrame()
    df_prospects = pd.DataFrame()
    df_vagas = pd.DataFrame()


# Opcional: resetar índice para ter coluna 'id'
if not df_applicants.empty:
    df_applicants.reset_index(inplace=True)
    df_applicants.rename(columns={"index": "id_candidato"}, inplace=True)
else:
    print("df_applicants está vazio, pulando reset_index e rename.")

if not df_vagas.empty:
    df_vagas.reset_index(inplace=True)
    df_vagas.rename(columns={"index": "id_vaga"}, inplace=True)
else:
    print("df_vagas está vazio, pulando reset_index e rename.")

# --- Correção para df_prospects: Explodir e Normalizar dados aninhados ---
if not df_prospects.empty:
    # Resetar o índice de df_prospects para que o ID da vaga se torne uma coluna
    df_prospects.reset_index(inplace=True)
    df_prospects.rename(columns={"index": "id_vaga"}, inplace=True)

    # Explodir a coluna 'prospects' para que cada candidato tenha sua própria linha
    df_prospects_exploded = df_prospects.explode("prospects")

    # Resetar o índice, mantendo o índice original como uma coluna 'temp_idx'
    df_prospects_exploded = df_prospects_exploded.reset_index().rename(columns={'index': 'temp_idx'})

    # Normalizar os dados aninhados na coluna 'prospects'
    # Esta operação pode falhar se 'prospects' não for uma lista de dicionários.
    try:
        df_prospects_normalized = pd.json_normalize(df_prospects_exploded["prospects"])
    except Exception as e:
        print(f"Erro ao normalizar coluna 'prospects': {e}. Verifique o formato dos dados em 'prospects'.")
        # Se houver um erro, crie um DataFrame vazio para evitar que o código quebre.
        df_prospects_normalized = pd.DataFrame()

    if not df_prospects_normalized.empty:
        # Adicionar a coluna 'temp_idx' ao df_prospects_normalized para merge
        df_prospects_normalized['temp_idx'] = df_prospects_exploded['temp_idx']

        # Mesclar os dois DataFrames usando 'temp_idx'
        df_prospects_final = pd.merge(
            df_prospects_exploded.drop(columns=["prospects"]),
            df_prospects_normalized,
            on='temp_idx',
            how='left'
        )

        # Remover a coluna temporária 'temp_idx' se não for mais necessária
        df_prospects_final.drop(columns=['temp_idx'], inplace=True)

        # Renomear a coluna 'codigo' para 'id_candidato' para permitir a junção com df_applicants
        df_prospects_final.rename(columns={"codigo": "id_candidato"}, inplace=True)

        # Remover linhas onde 'id_candidato' é nulo (ocorre se a lista de prospects estava vazia ou mal formatada)
        df_prospects_final.dropna(subset=["id_candidato"], inplace=True)

        # Converter 'id_candidato' para o mesmo tipo de dado que em df_applicants (se necessário)
        # É importante que 'id_candidato' em ambos os DFs tenham o mesmo tipo antes do merge.
        df_prospects_final["id_candidato"] = df_prospects_final["id_candidato"].astype(str)
    else:
        print("Normalização de 'prospects' resultou em um DataFrame vazio. df_prospects_final será vazio.")
        df_prospects_final = pd.DataFrame()
else:
    print("df_prospects está vazio, pulando processamento de prospects.")
    df_prospects_final = pd.DataFrame()

# Garantir que df_applicants['id_candidato'] seja string também para consistência nos merges futuros.
if not df_applicants.empty and 'id_candidato' in df_applicants.columns:
    df_applicants["id_candidato"] = df_applicants["id_candidato"].astype(str)


print("\n--- Validação do Carregamento e Estrutura ---")
print("Dados de candidatos (df_applicants - primeiras 5 linhas):")
print(df_applicants.head())
print(f"Shape de df_applicants: {df_applicants.shape}")
print(f"Colunas de df_applicants: {df_applicants.columns.tolist()}")

print("\nDados de candidaturas (df_prospects_final - após correção de estrutura - primeiras 5 linhas):")
print(df_prospects_final.head())
print(f"Shape de df_prospects_final: {df_prospects_final.shape}")
print(f"Colunas de df_prospects_final: {df_prospects_final.columns.tolist()}")

print("\nDados de vagas (df_vagas - primeiras 5 linhas):")
print(df_vagas.head())
print(f"Shape de df_vagas: {df_vagas.shape}")
print(f"Colunas de df_vagas: {df_vagas.columns.tolist()}")

print("\nCarregamento e pré-processamento dos dados concluídos.")

"""#3. Criação da variável alvo “sucesso”"""

# --- 3. Criação da variável alvo “sucesso” ---
print("\n### 3. Criação da variável alvo \"sucesso\"")

# Verifica se 'df_prospects_final' não está vazio antes de tentar criar a coluna 'sucesso'
if not df_prospects_final.empty:
    # A coluna 'situacao_candidado' agora é uma coluna de nível superior em df_prospects_final
    df_prospects_final["sucesso"] = df_prospects_final["situacao_candidado"].apply(
        lambda x: 1 if x == "Contratado pela Decision" else 0
    )
    print("Contagem de 'sucesso' (1=Contratado, 0=Não contratado):")
    print(df_prospects_final["sucesso"].value_counts())
else:
    print("DataFrame df_prospects_final está vazio. Não foi possível criar a variável 'sucesso'.")

"""#4. Junção dos DataFrames"""

# --- 4. Junção dos DataFrames ---
print("\n### 4. Junção dos DataFrames")

# Verifica se os DataFrames necessários não estão vazios antes de tentar o merge
if df_prospects_final.empty:
    print("Erro: df_prospects_final está vazio. Não é possível realizar a junção.")
    df = pd.DataFrame() # Cria um DataFrame vazio para evitar erros futuros
elif df_applicants.empty:
    print("Erro: df_applicants está vazio. Não é possível realizar a junção completa.")
    df = df_prospects_final.copy() # Continua com o que tem, mas informa o problema
elif df_vagas.empty:
    print("Erro: df_vagas está vazio. Não é possível realizar a junção completa.")
    df = df_prospects_final.merge(df_applicants, on="id_candidato", how="left") # Mescla apenas com applicants

else:
    # Mesclar candidaturas com informações de candidatos
    # Usa df_prospects_final que tem a estrutura corrigida e 'id_candidato' normalizado
    print("Realizando merge de df_prospects_final com df_applicants...")
    df = df_prospects_final.merge(df_applicants, on="id_candidato", how="left")

    # Realizando merge do resultado com informações de vagas
    # Adiciona sufixos para diferenciar colunas com o mesmo nome que vêm de diferentes fontes
    print("Realizando merge com df_vagas...")
    df = df.merge(df_vagas, on="id_vaga", how="left", suffixes=("_candidato","_vaga"))

    # Validação pós-merge: Verificar se existem valores nulos nos IDs após o merge
    # Isso pode indicar que um candidato ou vaga não foi encontrado em seus respectivos DataFrames
    if df['id_candidato'].isnull().any():
        print(f"Alerta: {df['id_candidato'].isnull().sum()} candidatos não encontrados em df_applicants após o merge.")
    if df['id_vaga'].isnull().any():
        print(f"Alerta: {df['id_vaga'].isnull().sum()} vagas não encontradas em df_vagas após o merge.")


print("\n--- Validação da Junção ---")
print("DataFrame final após junções (colunas resultantes):")
print(df.columns.tolist())
print("\nPrimeiras 2 linhas do DataFrame final:")
print(df.head(2))
print(f"\nShape do DataFrame final: {df.shape}")

print("\nJunção dos DataFrames concluída.")

"""#5. Tratamento de dados faltantes e codificação"""

# --- 5. Tratamento de dados faltantes e codificação ---
print("\n### 5. Tratamento de dados faltantes e codificação")

# Lista de colunas para verificar nulos antes do tratamento
# Estas são as colunas que esperamos ter sido criadas nas etapas anteriores
columns_to_check_for_nulls = [
    'id_candidato', 'id_vaga', 'sucesso',
    'pais_candidato', 'estado_candidato', 'cidade_candidato',
    'pais_vaga', 'estado_vaga', 'cidade_vaga',
    'idade_candidato', 'experiencia_candidato_anos',
    'pcd_candidato', 'vaga_pcd', # Adicionadas para mais cobertura
    'nivel_academico_candidato', 'nivel_academico_vaga',
    'nivel_ingles_candidato', 'nivel_ingles_vaga',
    'nivel_espanhol_candidato', 'nivel_espanhol_vaga'
]

# Filtrar apenas as colunas que realmente existem no DataFrame
existing_cols_for_null_check = [col for col in columns_to_check_for_nulls if col in df.columns]

print("\nValores nulos ANTES do tratamento (principais colunas existentes):")
if existing_cols_for_null_check:
    # Seleciona apenas as colunas existentes para evitar KeyError
    null_counts = df[existing_cols_for_null_check].isnull().sum()
    # Mostra apenas as colunas que realmente têm valores nulos
    print(null_counts[null_counts > 0])
else:
    print("Nenhuma das colunas esperadas para verificação de nulos existe no DataFrame.")
    print("Isso pode indicar um problema nas etapas de carregamento ou junção dos dados (Partes 2 e 4).")
    print(f"Colunas atuais no DataFrame: {df.columns.tolist()}")


# Exemplo de tratamento simples: eliminar linhas com dados faltantes
# ATENÇÃO: Isso pode remover muitas linhas. Considere imputação para colunas específicas.
initial_rows = df.shape[0]
df.dropna(inplace=True) # Usando inplace=True para modificar o DataFrame diretamente
rows_after_dropna = df.shape[0]
print(f"\nLinhas antes de dropna: {initial_rows}")
print(f"Linhas depois de dropna: {rows_after_dropna}")
print(f"Total de linhas removidas devido a NaN: {initial_rows - rows_after_dropna}")

# Se o DataFrame ficar vazio após dropna, precisamos avisar
if df.empty:
    print("\nAlerta: O DataFrame ficou vazio após a remoção de linhas com valores nulos.")
    print("Não é possível prosseguir com a codificação. Verifique a qualidade dos seus dados de entrada.")
else:
    # --- Identificação de Colunas Categóricas para Codificação One-Hot ---
    # É fundamental que você liste aqui TODAS as colunas categóricas que deseja codificar.
    # Nível Acadêmico, Níveis de Idioma, PCD, Local de Trabalho, etc.
    categorical_cols = [
        # Colunas de localização (se você não usar os 'match' binários diretamente)
        # "pais_candidato", "estado_candidato", "cidade_candidato", # Cuidado com cardinalidade alta
        # "pais_vaga", "estado_vaga", "cidade_vaga", # Cuidado com cardinalidade alta

        # Colunas de PCD
        "pcd_candidato",
        "vaga_pcd",

        # Nível Acadêmico
        "nivel_academico_candidato",
        "nivel_academico_vaga",

        # Idiomas (se você não usar apenas os 'match' binários)
        "nivel_ingles_candidato",
        "nivel_ingles_vaga",
        "nivel_espanhol_candidato",
        "nivel_espanhol_vaga",

        # Nível Profissional da Vaga
        "nivel_profissional_vaga",
        "local_trabalho_vaga",
        # Adicione outras colunas categóricas relevantes do seu df aqui.
        # Ex: "area_vaga", "tipo_contratacao", etc.
    ]

    # Remover 'situacao_candidado' da lista, pois 'sucesso' já foi criado a partir dela
    # e geralmente não se usa a variável original que gerou o target como feature.
    if "situacao_candidado" in df.columns and "situacao_candidado" in categorical_cols:
        categorical_cols.remove("situacao_candidado")

    # Filtrar apenas as colunas que realmente existem no DataFrame para evitar KeyError
    categorical_cols_existing = [col for col in categorical_cols if col in df.columns]

    if categorical_cols_existing:
        print(f"\nColunas categóricas para codificação one-hot: {categorical_cols_existing}")
        df = pd.get_dummies(df, columns=categorical_cols_existing, drop_first=True)

        print("\nDataFrame após codificação one-hot (cabeçalho com novas colunas):")
        dummy_cols_created = [col for col in df.columns if any(c + "_" in col for c in categorical_cols_existing)]
        # Limita o print das colunas para melhor visualização
        print(df[dummy_cols_created[:10] + ['sucesso', 'id_candidato', 'id_vaga']].head())
        print(f"\nTotal de novas colunas dummy criadas: {len(dummy_cols_created)}")
        print(f"Novo shape do DataFrame: {df.shape}")

    else:
        print("Nenhuma coluna categórica identificada e existente no DataFrame para codificação one-hot nesta etapa.")

print("\nTratamento de dados faltantes e codificação concluídos.")

"""#6. Engenharia de atributos

##6.1
"""

import pandas as pd
import numpy as np
from datetime import datetime
import re
import unicodedata
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# --- Garantir download do NLTK punkt e punkt_tab ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Baixando recurso 'punkt' do NLTK...")
    nltk.download('punkt', quiet=True)
    print("Recurso 'punkt' baixado com sucesso.")

try:
    nltk.data.find('tokenizers/punkt_tab/portuguese.pickle')
except LookupError:
    print("Baixando recurso 'punkt_tab' do NLTK (para português)...")
    nltk.download('punkt_tab', quiet=True)
    print("Recurso 'punkt_tab' baixado com sucesso.")

# Definir stop_words (se ainda não estiver definido globalmente)
try:
    _ = stopwords.words("portuguese")
except LookupError:
    nltk.download("stopwords", quiet=True)
stop_words = set(stopwords.words("portuguese"))


# --- Funções auxiliares (mantidas e ajustadas) ---
def standardize_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8", "ignore")
    text = re.sub(r'[^a-z0-9\s]', '', text)
    return text.strip()

def clean_and_tokenize(text, min_word_len=5):
    text = standardize_text(text)
    if not text:
        return []
    tokens = word_tokenize(text, language='portuguese')
    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_word_len]
    return tokens

def extract_experience_years(cv_text):
    if not isinstance(cv_text, str):
        return 0
    matches = re.findall(r'\d{2}/\d{2}/\d{4}\s*(?:[áa])\s*(?:atual|\d{2}/\d{2}/\d{4})', cv_text, re.IGNORECASE)
    total_experience_days = 0
    for match in matches:
        parts = re.split(r'\s*[áa]\s*', match)
        start_date_str = parts[0].strip()
        end_date_str = parts[1].strip()
        try:
            start_date = datetime.strptime(start_date_str, '%d/%m/%Y')
            end_date = datetime.now() if end_date_str.lower() == 'atual' else datetime.strptime(end_date_str, '%d/%m/%Y')
            duration = end_date - start_date
            total_experience_days += duration.days
        except ValueError:
            continue
    return total_experience_days / 365.25

def map_nivel_profissional_to_experience(nivel):
    nivel_map = {
        'estagiário': 0, 'junior': 1, 'pleno': 3, 'senior': 5,
        'especialista': 8, 'gerente': 10, 'diretor': 12
    }
    return nivel_map.get(standardize_text(nivel), 0)

def get_nested_value(row, path, default=None):
    keys = path.split(".")
    value = row
    for i, key in enumerate(keys):
        if isinstance(value, dict) and key in value:
            value = value[key]
        elif isinstance(value, pd.Series) and key in value.index:
            value = value[key]
        else:
            return default
    return value

# --- Início da Parte 6: Engenharia de Atributos ---
print("\n### 6. Engenharia de atributos")

if 'df' not in locals() or df.empty:
    print("O DataFrame 'df' está vazio ou não foi carregado. Não é possível realizar a engenharia de atributos.")
else:
    # --- Lógica para Extração de Localização do Candidato ---
    def extract_candidate_location(row, location_type):
        location_str = get_nested_value(row, "infos_basicas.local", "")
        if isinstance(location_str, str) and location_str.strip():
            standardized_parts = [p for p in re.split(r'[\s,\-]+', standardize_text(location_str)) if p]

            if location_type == 'pais':
                if 'brasil' in standardized_parts: return 'brasil'
                return 'brasil'

            elif location_type == 'estado':
                if 'rio' in standardized_parts and 'de' in standardized_parts and 'janeiro' in standardized_parts:
                    return 'rio de janeiro'
                if 'sao' in standardized_parts and 'paulo' in standardized_parts:
                    return 'sao paulo'
                if len(standardized_parts) >= 2 and standardized_parts[0] == standardized_parts[1] and len(standardized_parts[0]) > 2:
                    return standardized_parts[0]

                if len(standardized_parts[-1]) == 2 and standardized_parts[-1] in ["sp", "rj", "mg", "ba", "rs", "pr", "sc", "df", "go", "es", "ce", "pe", "pa", "am", "al", "ap", "pb", "pi", "rn", "se", "to", "ro", "rr", "ac", "ma", "ms", "mt"]:
                    return standardized_parts[-1]

                if len(standardized_parts) > 1 and standardized_parts[1] not in ['de', 'da', 'do']:
                    return standardized_parts[1]
                return ""

            elif location_type == 'cidade':
                if standardized_parts:
                    if 'rio' in standardized_parts and 'de' in standardized_parts and 'janeiro' in standardized_parts:
                        return 'rio de janeiro'
                    if 'sao' in standardized_parts and 'paulo' in standardized_parts:
                        return 'sao paulo'
                    return standardized_parts[0]
                return ""

        address_str = standardize_text(get_nested_value(row, "informacoes_pessoais.endereco", ""))
        if address_str:
            if location_type == 'cidade':
                return address_str
            elif location_type == 'estado':
                if 'sao paulo' in address_str: return 'sao paulo'
                if 'rio de janeiro' in address_str: return 'rio de janeiro'
                return ""
            elif location_type == 'pais':
                return 'brasil'

        return ""

    df["pais_candidato"] = df.apply(lambda row: extract_candidate_location(row, "pais"), axis=1)
    df["estado_candidato"] = df.apply(lambda row: extract_candidate_location(row, "estado"), axis=1)
    df["cidade_candidato"] = df.apply(lambda row: extract_candidate_location(row, "cidade"), axis=1)

    df["pais_vaga"] = df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.pais", "")), axis=1)
    df["estado_vaga"] = df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.estado", "")), axis=1)
    df["cidade_vaga"] = df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.cidade", "")), axis=1)
    df["local_trabalho_vaga"] = df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.local_trabalho", "")), axis=1)

    print("\nValores extraídos para Candidato (Localização):")
    print(df[["pais_candidato", "estado_candidato", "cidade_candidato"]].head())
    print("\nValores extraídos para Vaga (Localização):")
    print(df[["pais_vaga", "estado_vaga", "cidade_vaga", "local_trabalho_vaga"]].head())

    df["match_pais"] = (df["pais_candidato"] == df["pais_vaga"]).astype(int)
    df["match_estado"] = (df["estado_candidato"] == df["estado_vaga"]).astype(int)
    df["match_cidade"] = (df["cidade_candidato"] == df["cidade_vaga"]).astype(int)
    print("\nResultados do Match de Localização:")
    print(df[["match_pais", "match_estado", "match_cidade"]].head())
    print(df["match_pais"].value_counts(dropna=False))
    print(df["match_estado"].value_counts(dropna=False))
    print(df["match_cidade"].value_counts(dropna=False))


    print("\n--- Validação: PCD ---")
    df["pcd_candidato"] = df.apply(lambda row: standardize_text(get_nested_value(row, "informacoes_pessoais.pcd", "")), axis=1)
    df["vaga_pcd"] = df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.vaga_especifica_para_pcd", "")), axis=1)
    print("Valores extraídos para PCD:")
    print(df[["pcd_candidato", "vaga_pcd"]].head())
    df["match_pcd"] = ((df["vaga_pcd"] == "sim") & (df["pcd_candidato"] == "sim")).astype(int)
    print("\nResultado do Match PCD:")
    print(df["match_pcd"].head())
    print(df["match_pcd"].value_counts(dropna=False))


    print("\n--- Validação: Faixa Etária ---")
    def calculate_age(dob_str):
        try:
            if isinstance(dob_str, str):
                if dob_str == "0000-00-00":
                    return np.nan
                if re.match(r'\d{2}-\d{2}-\d{4}', dob_str):
                    dob = datetime.strptime(dob_str, "%d-%m-%Y")
                elif re.match(r'\d{4}-\d{2}-\d{2}', dob_str):
                    dob = datetime.strptime(dob_str, "%Y-%m-%d")
                else:
                    return np.nan
            else:
                return np.nan
            # Trunca a idade para o número inteiro, descartando as casas decimais
            return int((datetime.now() - dob).days / 365.25)
        except (ValueError, TypeError):
            return np.nan

    def check_age_match(age, faixa_etaria_str):
        if pd.isna(age) or not isinstance(faixa_etaria_str, str):
            return 0

        faixa_etaria_str_standardized = standardize_text(faixa_etaria_str)
        numbers = re.findall(r'\d+', faixa_etaria_str_standardized)

        if len(numbers) >= 2:
            try:
                min_age = int(numbers[0])
                max_age = int(numbers[1])
                return int(age >= min_age and age <= max_age)
            except ValueError:
                return 0
        return 0

    df["idade_candidato"] = df.apply(lambda row: calculate_age(get_nested_value(row, "informacoes_pessoais.data_nascimento", "")), axis=1)
    df["faixa_etaria_vaga"] = df.apply(lambda row: get_nested_value(row, "perfil_vaga.faixa_etaria", ""), axis=1)

    print("Valores extraídos para Faixa Etária:")
    print(df[["idade_candidato", "faixa_etaria_vaga"]].head())

    df["match_faixa_etaria"] = df.apply(lambda row: check_age_match(row["idade_candidato"], row["faixa_etaria_vaga"]), axis=1)
    print("\nResultado do Match Faixa Etária:")
    print(df["match_faixa_etaria"].head())
    print(df["match_faixa_etaria"].value_counts(dropna=False))


    print("\n--- Validação: Nível Acadêmico ---")
    df["nivel_academico_candidato"] = df.apply(lambda row: standardize_text(get_nested_value(row, "formacao_e_idiomas.nivel_academico", "")), axis=1)
    df["nivel_academico_vaga"] = df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.nivel_academico", "")), axis=1)
    print("Valores extraídos para Nível Acadêmico:")
    print(df[["nivel_academico_candidato", "nivel_academico_vaga"]].head())
    df["match_nivel_academico"] = (df["nivel_academico_candidato"] == df["nivel_academico_vaga"]).astype(int)
    print("\nResultado do Match Nível Acadêmico:")
    print(df["match_nivel_academico"].head())
    print(df["match_nivel_academico"].value_counts(dropna=False))


    print("\n--- Validação: Idiomas ---")
    def check_language_match(candidato_level, vaga_level):
        candidato_level = standardize_text(candidato_level)
        vaga_level = standardize_text(vaga_level)

        levels = {"nenhum": 0, "basico": 1, "intermediario": 2, "avancado": 3, "fluente": 4}

        cand_val = levels.get(candidato_level, 0)
        vaga_val = levels.get(vaga_level, 0)

        if vaga_val == 0:
            return 1

        return int(cand_val >= vaga_val)

    df["nivel_ingles_candidato"] = df.apply(lambda row: get_nested_value(row, "formacao_e_idiomas.nivel_ingles", ""), axis=1)
    df["nivel_espanhol_candidato"] = df.apply(lambda row: get_nested_value(row, "formacao_e_idiomas.nivel_espanhol", ""), axis=1)

    df["nivel_ingles_vaga"] = df.apply(lambda row: get_nested_value(row, "perfil_vaga.nivel_ingles", ""), axis=1)
    df["nivel_espanhol_vaga"] = df.apply(lambda row: get_nested_value(row, "perfil_vaga.nivel_espanhol", ""), axis=1)

    print("Valores extraídos para Inglês:")
    print(df[["nivel_ingles_candidato", "nivel_ingles_vaga"]].head())
    df["match_ingles"] = df.apply(lambda row: check_language_match(row["nivel_ingles_candidato"], row["nivel_ingles_vaga"]), axis=1)
    print("\nResultado do Match Inglês:")
    print(df["match_ingles"].head())
    print(df["match_ingles"].value_counts(dropna=False))

    print("\nValores extraídos para Espanhol:")
    print(df[["nivel_espanhol_candidato", "nivel_espanhol_vaga"]].head())
    df["match_espanhol"] = df.apply(lambda row: check_language_match(row["nivel_espanhol_candidato"], row["nivel_espanhol_vaga"]), axis=1)
    print("\nResultado do Match Espanhol:")
    print(df["match_espanhol"].head())
    print(df["match_espanhol"].value_counts(dropna=False))


    print("\n--- Validação: Certificações (Contagem) ---")
    def count_certifications(cert_str):
        if not isinstance(cert_str, str) or not cert_str.strip():
            return 0
        certs = [standardize_text(c) for c in cert_str.split(",") if standardize_text(c)]
        return len(certs)

    df["num_certificacoes_candidato"] = df.apply(lambda row: count_certifications(get_nested_value(row, "informacoes_profissionais.certificacoes", "")), axis=1)
    df["num_outras_certificacoes_candidato"] = df.apply(lambda row: count_certifications(get_nested_value(row, "informacoes_profissionais.outras_certificacoes", "")), axis=1)

    print("Resultados da Contagem de Certificações:")
    print(df[["num_certificacoes_candidato", "num_outras_certificacoes_candidato"]].head())
    print(df["num_certificacoes_candidato"].value_counts(dropna=False))
    print(df["num_outras_certificacoes_candidato"].value_counts(dropna=False))


    print("\n--- Validação: Experiência ---")
    df["experiencia_candidato_anos"] = df.apply(lambda row: extract_experience_years(get_nested_value(row, "cv_pt", "")), axis=1)

    df["nivel_profissional_vaga"] = df.apply(lambda row: get_nested_value(row, "perfil_vaga.nivel profissional", "") if 'perfil_vaga' in row and isinstance(row['perfil_vaga'], dict) else "", axis=1)
    df["experiencia_vaga_anos"] = df["nivel_profissional_vaga"].apply(map_nivel_profissional_to_experience)

    print("Valores extraídos para Experiência:")
    print(df[["experiencia_candidato_anos", "nivel_profissional_vaga", "experiencia_vaga_anos"]].head())

    df["dif_experiencia"] = df["experiencia_candidato_anos"] - df["experiencia_vaga_anos"]
    df["atende_experiencia"] = (df["experiencia_candidato_anos"] >= df["experiencia_vaga_anos"]).astype(int)

    print("\nResultados do Match de Experiência:")
    print(df[["dif_experiencia", "atende_experiencia"]].head())
    print(df["atende_experiencia"].value_counts(dropna=False))


    # --- Removendo candidatos sem dados essenciais ---
    print("\n--- Removendo candidatos sem informações essenciais ---")
    initial_rows = len(df)

    # Condições para REMOVER uma linha (se *todas* forem verdadeiras para o candidato)
    # 1. Localização do candidato é totalmente vazia (país, estado e cidade)
    # 2. Idade do candidato é NaN
    # 3. Nível acadêmico do candidato é vazio
    # 4. Não tem experiência profissional (0 anos)

    is_location_empty = (df["pais_candidato"] == "") & (df["estado_candidato"] == "") & (df["cidade_candidato"] == "")
    is_age_nan = df["idade_candidato"].isna()
    is_academic_level_empty = (df["nivel_academico_candidato"] == "")
    is_experience_zero = (df["experiencia_candidato_anos"] == 0)

    rows_to_remove_mask = is_location_empty & is_age_nan & is_academic_level_empty & is_experience_zero

    df = df[~rows_to_remove_mask].copy()

    rows_removed = initial_rows - len(df)
    print(f"Número de linhas de candidatos removidas devido a informações essenciais incompletas: {rows_removed}")
    print(f"Shape do DataFrame após a remoção: {df.shape}")

    print("\nAtributos de engenharia de features criados com sucesso na Parte 6.")
    print("\n--- Validação Final do DataFrame (Parte 6) ---")
    print(f"Shape final do DataFrame após Parte 6: {df.shape}")
    print("Colunas finais no DataFrame após Parte 6:")
    print(df.columns.tolist())
    print("\nPrimeiras 5 linhas do DataFrame final com as novas features (após remoção):")
    print(df.head())

"""##6.2"""

import pandas as pd
import numpy as np
import re
import unicodedata
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import gc # Para o coletor de lixo

# --- Garante que NLTK downloads e stopwords estejam definidos (necessário se rodar este bloco isoladamente) ---
# Você pode comentar este trecho se tiver certeza que as configurações NLTK e stop_words já foram feitas em outro lugar.
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('tokenizers/punkt_tab/portuguese.pickle')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

try:
    _ = stopwords.words("portuguese")
except LookupError:
    nltk.download("stopwords", quiet=True)
stop_words = set(stopwords.words("portuguese"))


# --- Funções auxiliares (repetidas aqui para garantir que 6.2 rode sozinho, mas idealmente estariam em um bloco de funções gerais) ---
# Se estas funções já estiverem definidas no seu script (como no bloco 6.1 que você forneceu),
# você pode REMOVER ESTAS DEFINIÇÕES DAQUI para evitar duplicação.
def standardize_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8", "ignore")
    text = re.sub(r'[^a-z0-9\s]', '', text)
    return text.strip()

def clean_and_tokenize(text, min_word_len=5):
    text = standardize_text(text)
    if not text:
        return []
    tokens = word_tokenize(text, language='portuguese')
    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_word_len]
    return tokens

def get_nested_value(row, path, default=None):
    keys = path.split(".")
    value = row
    for i, key in enumerate(keys):
        if isinstance(value, dict) and key in value:
            value = value[key]
        elif isinstance(value, pd.Series) and key in value.index:
            value = value[key]
        else:
            return default
    return value


# --- Início da Parte 6.2: Habilidades/Atividades ---
print("\n### 6.2. Habilidades/Atividades (Processamento em Chunks - Otimizado para RAM)")

if 'df' not in locals() or df.empty:
    print("O DataFrame 'df' está vazio ou não foi carregado. Não é possível realizar a engenharia de atributos de habilidades.")
else:
    # Verifica se as colunas necessárias para as habilidades existem
    if 'cv_pt' in df.columns and 'cv_en' in df.columns:
        print("Iniciando processamento de Habilidades/Atividades em chunks. Isso pode levar tempo, mas com menor consumo de RAM.")

        # Função que encapsula toda a lógica de habilidades para ser aplicada por linha
        def calculate_skills_match_single_row(row):
            vaga_text = str(get_nested_value(row, "perfil_vaga.principais_atividades", "")) + " " + \
                        str(get_nested_value(row, "perfil_vaga.competencia_tecnicas_e_comportamentais", ""))
            vaga_tokens = set(clean_and_tokenize(vaga_text, min_word_len=5))

            candidato_text = str(row.get("cv_pt", "")) + " " + str(row.get("cv_en", ""))
            candidato_tokens = set(clean_and_tokenize(candidato_text, min_word_len=5))

            num_comum = len(vaga_tokens.intersection(candidato_tokens))
            match = int(num_comum >= 4)

            return num_comum, match

        # Listas para armazenar os resultados de cada chunk
        all_num_habilidades_em_comum = []
        all_match_habilidades = []

        # Define o tamanho do chunk. AJUSTE ESTE VALOR conforme a memória disponível.
        # Comece com um valor menor (ex: 5000 ou 10000) e aumente se funcionar sem estourar a RAM.
        chunk_size = 20000 # Exemplo: processa 20.000 linhas por vez

        num_chunks = (len(df) + chunk_size - 1) // chunk_size # Calcula o número total de chunks

        for i in range(num_chunks):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, len(df))

            # Seleciona o chunk atual do DataFrame
            current_chunk = df.iloc[start_idx:end_idx]

            print(f"Processando chunk {i+1}/{num_chunks} (linhas {start_idx} a {end_idx-1})...")

            # Aplica a função no chunk e obtém os resultados
            chunk_results = current_chunk.apply(calculate_skills_match_single_row, axis=1, result_type='expand')

            # Renomeia as colunas do resultado do chunk
            chunk_results.columns = ['num_habilidades_em_comum', 'match_habilidades']

            # Armazena os resultados do chunk
            all_num_habilidades_em_comum.append(chunk_results['num_habilidades_em_comum'])
            all_match_habilidades.append(chunk_results['match_habilidades'])

            # Limpa a memória: deleta o chunk e seus resultados e força a coleta de lixo
            del current_chunk
            del chunk_results
            gc.collect()

        # Concatena todos os resultados dos chunks
        df['num_habilidades_em_comum'] = pd.concat(all_num_habilidades_em_comum)
        df['match_habilidades'] = pd.concat(all_match_habilidades)

        print("\nProcessamento de Habilidades/Atividades concluído para todos os chunks.")

        print("\nResultados do Match de Habilidades:")
        print(df["num_habilidades_em_comum"].head())
        print(df["num_habilidades_em_comum"].value_counts(bins=5, dropna=False).sort_index())
        print(df["match_habilidades"].value_counts(dropna=False))

    else:
        print("Alerta: Colunas 'cv_pt' ou 'cv_en' não encontradas para engenharia de features de habilidades.")
        df["num_habilidades_em_comum"] = 0
        df["match_habilidades"] = 0

    print("\nEngenharia de features de Habilidades/Atividades concluída.")
    print("\n--- Validação Final do DataFrame (Parte 6.2) ---")
    print(f"Shape final do DataFrame após Parte 6.2: {df.shape}")
    print("Colunas finais no DataFrame após Parte 6.2:")
    print(df.columns.tolist())
    print("\nPrimeiras 5 linhas do DataFrame final com as novas features (após habilidades):")
    print(df.head())

"""#7. Preparação dos conjuntos de treino e teste"""

import pandas as pd
from sklearn.model_selection import train_test_split # Importação CORRIGIDA
# Certifique-se de que 'df' está disponível no seu ambiente.
# Exemplo de carregamento, APENAS SE df ainda não estiver carregado:
# df = pd.read_csv('seu_dataframe_processado.csv') # Ajuste para o seu formato de arquivo real

print("\n### 7. Preparação dos conjuntos de treino e teste")

if 'df' not in locals() or df.empty:
    print("O DataFrame 'df' está vazio ou não foi carregado. Não é possível preparar os conjuntos de treino e teste.")
else:
    print(f"DataFrame 'df' carregado com {df.shape[0]} linhas e {df.shape[1]} colunas para a Parte 7.")

    # --- Definir a variável alvo (y) ---
    # É crucial que 'sucesso' seja a coluna correta para a variável alvo.
    if 'sucesso' not in df.columns:
        print("Erro: A coluna 'sucesso' (variável alvo) não foi encontrada no DataFrame. Verifique as etapas anteriores.")
        # Opcional: Se 'sucesso' não existe, você pode tentar criar uma dummy para continuar ou abortar.
        # Por exemplo, se 'situacao_candidado' deve ser usada para criar 'sucesso':
        # df['sucesso'] = (df['situacao_candidado'] == 'Contratado').astype(int) # Exemplo de criação

    y = df["sucesso"]
    print(f"Variável alvo 'y' criada com {y.shape[0]} elementos. Distribuição:\n{y.value_counts()}")

    # --- Definir as colunas para X (features) ---
    # Lista de colunas para remover de X (features).
    # Mantive sua lista original e adicionei mais alguns que provavelmente não seriam features.
    colunas_para_remover_de_X = [
        "id_candidato", "id_vaga", "titulo", "modalidade",
        "nome", "data_candidatura", "ultima_atualizacao", "comentario", "recrutador",
        "id_candidato_candidato", "nome_candidato",
        "id_vaga_vaga", "nome_vaga",
        "pais_candidato", "estado_candidato", "cidade_candidato",
        "pais_vaga", "estado_vaga", "cidade_vaga", "local_trabalho_vaga",
        "pcd_candidato", "vaga_pcd",
        "idade_candidato", "faixa_etaria_vaga",
        "nivel_academico_candidato", "nivel_academico_vaga",
        "nivel_ingles_candidato", "nivel_ingles_vaga",
        "nivel_espanhol_candidato", "nivel_espanhol_vaga",
        "nivel_profissional_vaga",
        "cv_pt", "cv_en",
        "vaga_tokens", "candidato_tokens", # Se estas foram criadas, remova
        # Colunas JSON aninhadas originais, que foram expandidas em features
        "informacoes_pessoais", "infos_basicas", "informacoes_profissionais",
        "formacao_e_idiomas", "cargo_atual",
        "informacoes_basicas", "perfil_vaga", "beneficios"
    ]

    # Adicionar 'situacao_candidado' à lista de remoção se existir e 'sucesso' já estiver definida.
    if "situacao_candidado" in df.columns and "sucesso" in df.columns:
        if "situacao_candidado" not in colunas_para_remover_de_X:
            colunas_para_remover_de_X.append("situacao_candidado")

    # Filtrar apenas as colunas que REALMENTE existem no DataFrame para evitar erros
    colunas_existentes_para_remover = [col for col in colunas_para_remover_de_X if col in df.columns]

    # Excluir a variável alvo 'sucesso' explicitamente de X
    # Usamos errors='ignore' para que não dê erro se a coluna já tiver sido tratada ou não existir.
    X = df.drop(columns=colunas_existentes_para_remover + ["sucesso"], errors="ignore")

    print(f"\nDataFrame X (features) criado com {X.shape[0]} linhas e {X.shape[1]} colunas.")
    print("Exemplo das primeiras colunas de X:")
    print(X.head())

    # --- Tratamento de valores não numéricos em X ---
    # Antes de dividir, é CRUCIAL que X contenha apenas dados numéricos.
    # Esta etapa converterá quaisquer colunas não numéricas remanescentes (como strings) para numéricas,
    # ou as removerá se não puderem ser convertidas (por exemplo, texto livre).
    # Melhor abordagem: OHE para categóricas, remoção para texto que não foi processado.

    # Identificar colunas não numéricas remanescentes
    non_numeric_cols = X.select_dtypes(include=['object', 'category']).columns

    if not non_numeric_cols.empty:
        print(f"\nDetectadas colunas não numéricas em X. Será aplicada One-Hot Encoding ou remoção.")
        print(f"Colunas não numéricas: {non_numeric_cols.tolist()}")

        # Opção 1: One-Hot Encode para colunas categóricas com poucos valores únicos
        # Limite o número de categorias para evitar explosão de colunas
        for col in non_numeric_cols:
            if X[col].nunique() < 50: # Ajuste este limite conforme a sua necessidade
                # Usar pd.get_dummies para One-Hot Encoding.
                # `dummy_na=False` para não criar coluna para NaN, `drop_first=True` para evitar multicolinearidade.
                dummies = pd.get_dummies(X[col], prefix=col, dummy_na=False, drop_first=True)
                X = pd.concat([X, dummies], axis=1)
                X = X.drop(columns=[col]) # Remove a coluna original após o OHE
                print(f"  - Coluna '{col}' foi One-Hot Encoded.")
            else:
                # Opção 2: Remover colunas com muitos valores únicos ou texto livre não numérico
                # Se uma coluna `object` tem muitos valores únicos, provavelmente é texto ou ID e não deve ser OHE.
                # Se não foi processada para virar uma feature numérica, deve ser removida aqui.
                X = X.drop(columns=[col])
                print(f"  - Coluna '{col}' removida de X devido a muitos valores únicos ou tipo incompatível para OHE.")
    else:
        print("\nTodas as colunas em X são numéricas.")

    # Preencher quaisquer NaNs restantes em X com a média (ou outra estratégia)
    # Isso é crucial para muitos modelos de ML.
    if X.isnull().sum().sum() > 0:
        print("\nDetectados valores NaN em X. Preenchendo com a média das colunas numéricas.")
        # Para colunas numéricas, preenche com a média
        for col in X.select_dtypes(include=np.number).columns:
            if X[col].isnull().any():
                X[col] = X[col].fillna(X[col].mean())

        # Para colunas que ainda podem ser não numéricas após o OHE e contêm NaN (improvável se OHE foi aplicado),
        # pode-se preencher com uma string vazia ou um valor 'desconhecido' e depois re-processar.
        # Ou, para simplicidade, remover linhas com NaN restantes após tentar preencher numéricas.
        # Ou simplesmente remover colunas com qualquer NaN remanescente que não seja numérico.

        # Verificação final: se ainda houver NaN, pode ser um problema com o tipo da coluna ou processamento.
        # Para garantir, vou verificar e avisar, mas a estratégia de preenchimento acima lida com a maioria.
        if X.isnull().sum().sum() > 0:
            print("Atenção: Ainda existem valores NaN em X após o preenchimento da média. Verifique as colunas:")
            print(X.columns[X.isnull().any()].tolist())
            # Última medida de segurança: remover linhas com NaN restantes, mas prefira preencher se possível.
            # X = X.dropna()
            # y = y.loc[X.index] # Garantir que y corresponda

    # --- Divisão em treino e teste ---
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        test_size=0.20,
                                                        random_state=42,
                                                        stratify=y) # Adicionado stratify para lidar com desbalanceamento
                                                                    # Garante que a proporção de classes em y seja mantida em treino e teste.

    print(f"\nDivisão dos dados concluída.")
    print(f"Tamanho do conjunto de treino (X_train): {X_train.shape[0]} linhas, {X_train.shape[1]} colunas.")
    print(f"Tamanho do conjunto de teste (X_test): {X_test.shape[0]} linhas, {X_test.shape[1]} colunas.")
    print(f"Distribuição da variável alvo em y_train:\n{y_train.value_counts(normalize=True)}")
    print(f"Distribuição da variável alvo em y_test:\n{y_test.value_counts(normalize=True)}")

    print("\nPreparação dos conjuntos de treino e teste concluída com sucesso na Parte 7.")

"""#8. Treinamento do modelo Random Forest"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier # Importação CORRIGIDA

# Assegure-se de que X_train e y_train foram definidos pelas etapas anteriores (Parte 7)

print("\n### 8. Treinamento do modelo Random Forest")

# Verificar se X_train e y_train estão disponíveis e não vazios
if 'X_train' not in locals() or X_train.empty or 'y_train' not in locals() or y_train.empty:
    print("Erro: X_train ou y_train não estão disponíveis ou estão vazios. Certifique-se de que a Parte 7 foi executada corretamente.")
else:
    print(f"Dados de treino disponíveis: X_train com {X_train.shape[0]} linhas e y_train com {y_train.shape[0]} elementos.")

    # Criar e treinar o modelo RandomForestClassifier
    # n_estimators: número de árvores na floresta (100 é um bom ponto de partida)
    # random_state: semente para reprodutibilidade (garante os mesmos resultados em múltiplas execuções)
    rf = RandomForestClassifier(n_estimators=100, random_state=42)

    print("Iniciando treinamento do Modelo Random Forest...")
    rf.fit(X_train, y_train)
    print("Modelo Random Forest treinado com sucesso!")

    # Opcional: Você pode imprimir algumas informações básicas do modelo treinado
    print(f"\nNúmero de árvores no modelo: {rf.n_estimators}")
    print(f"Importância das features (top 5):")
    # Certifique-se de que X_train.columns está disponível para mapear a importância
    if hasattr(rf, 'feature_importances_') and len(rf.feature_importances_) == X_train.shape[1]:
        feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)
        print(feature_importances.nlargest(5))
    else:
        print("Não foi possível calcular a importância das features (verifique o shape de X_train ou se o modelo tem 'feature_importances_').")

    print("\nEtapa de treinamento do modelo concluída com sucesso na Parte 8.")

"""#9. Avaliação do modelo"""

import pandas as pd
# Importações das métricas do Scikit-learn
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report

# Assegure-se de que 'rf' (modelo treinado), 'X_test', e 'y_test'
# foram definidos pelas etapas anteriores (Partes 8 e 7, respectivamente).

print("\n### 9. Avaliação do modelo")

# Verificar se o modelo e os dados de teste estão disponíveis
if 'rf' not in locals():
    print("Erro: O modelo 'rf' não foi treinado. Certifique-se de que a Parte 8 foi executada corretamente.")
elif 'X_test' not in locals() or X_test.empty or 'y_test' not in locals() or y_test.empty:
    print("Erro: X_test ou y_test não estão disponíveis ou estão vazios. Certifique-se de que a Parte 7 foi executada corretamente.")
else:
    print(f"Dados de teste disponíveis: X_test com {X_test.shape[0]} linhas e y_test com {y_test.shape[0]} elementos.")

    # Fazer previsões no conjunto de teste
    print("Fazendo previsões no conjunto de teste...")
    y_pred = rf.predict(X_test)
    # A probabilidade da classe positiva (geralmente a classe 1) é a segunda coluna do array retornado por predict_proba.
    y_proba = rf.predict_proba(X_test)[:, 1]
    print("Previsões realizadas com sucesso.")

    # Calcular métricas de avaliação
    print("\nCalculando métricas de avaliação:")
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_proba) # Para AUC-ROC, passamos as probabilidades

    print(f"Acurácia: {acc:.3f}")
    print(f"F1-Score: {f1:.3f}")
    print(f"AUC-ROC: {roc:.3f}\n")

    # Exibir relatório de classificação detalhado
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    print("\nEtapa de avaliação do modelo concluída com sucesso na Parte 9.")

"""#10. Salvando o modelo treinado"""

import joblib # Importação CORRIGIDA (ou from joblib import dump)

# Assegure-se de que 'rf' (o modelo treinado) foi definido pelas etapas anteriores (Parte 8).

print("\n### 10. Salvando o modelo treinado")

# Verificar se o modelo 'rf' está disponível
if 'rf' not in locals():
    print("Erro: O modelo 'rf' não está disponível para ser salvo. Certifique-se de que a Parte 8 foi executada e o modelo foi treinado.")
else:
    # Definir o nome do arquivo para salvar o modelo
    modelo_filename = "modelo_compatibilidade.joblib"

    print(f"Tentando salvar o modelo em '{modelo_filename}'...")
    try:
        # Salvar o modelo em arquivo usando joblib.dump
        joblib.dump(rf, modelo_filename)
        print(f"Modelo salvo com sucesso em '{modelo_filename}'.")
    except Exception as e:
        print(f"Erro ao salvar o modelo: {e}")

print("\nEtapa de salvamento do modelo concluída na Parte 10.")

"""#11. Exemplo de previsão com novos dados"""

import pandas as pd
import numpy as np
from datetime import datetime
import re
import unicodedata
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import joblib # Para carregar o modelo

# --- Funções Auxiliares (Re-importar ou garantir que estejam no escopo) ---
# Estas funções DEVEM SER DEFINIDAS ou importadas ANTES deste bloco.
# Se você as definiu no início do seu script ou em um bloco de "funções auxiliares",
# elas já devem estar disponíveis. Caso contrário, inclua-as aqui.

# Configurações NLTK e stopwords (essencial)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('tokenizers/punkt_tab/portuguese.pickle')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

try:
    _ = stopwords.words("portuguese")
except LookupError:
    nltk.download("stopwords", quiet=True)
stop_words = set(stopwords.words("portuguese"))

# Funções de texto (essencial)
def standardize_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8", "ignore")
    text = re.sub(r'[^a-z0-9\s]', '', text)
    return text.strip()

def clean_and_tokenize(text, min_word_len=5):
    text = standardize_text(text)
    if not text:
        return []
    tokens = word_tokenize(text, language='portuguese')
    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_word_len]
    return tokens

def get_nested_value(row, path, default=None):
    keys = path.split(".")
    value = row
    for i, key in enumerate(keys):
        if isinstance(value, dict) and key in value:
            value = value[key]
        elif isinstance(value, pd.Series) and key in value.index:
            value = value[key]
        else:
            return default
    return value

def extract_experience_years(cv_text):
    if not isinstance(cv_text, str):
        return 0
    matches = re.findall(r'\d{2}/\d{2}/\d{4}\s*(?:[áa])\s*(?:atual|\d{2}/\d{2}/\d{4})', cv_text, re.IGNORECASE)
    total_experience_days = 0
    for match in matches:
        parts = re.split(r'\s*[áa]\s*', match)
        start_date_str = parts[0].strip()
        end_date_str = parts[1].strip()
        try:
            start_date = datetime.strptime(start_date_str, '%d/%m/%Y')
            end_date = datetime.now() if end_date_str.lower() == 'atual' else datetime.strptime(end_date_str, '%d/%m/%Y')
            duration = end_date - start_date
            total_experience_days += duration.days
        except ValueError:
            continue
    return total_experience_days / 365.25

def map_nivel_profissional_to_experience(nivel):
    nivel_map = {
        'estagiário': 0, 'junior': 1, 'pleno': 3, 'senior': 5,
        'especialista': 8, 'gerente': 10, 'diretor': 12
    }
    return nivel_map.get(standardize_text(nivel), 0)

def calculate_age(dob_str):
    try:
        if isinstance(dob_str, str):
            if dob_str == "0000-00-00":
                return np.nan
            if re.match(r'\d{2}-\d{2}-\d{4}', dob_str):
                dob = datetime.strptime(dob_str, "%d-%m-%Y")
            elif re.match(r'\d{4}-\d{2}-\d{2}', dob_str):
                dob = datetime.strptime(dob_str, "%Y-%m-%d")
            else:
                return np.nan
        else:
            return np.nan
        return int((datetime.now() - dob).days / 365.25)
    except (ValueError, TypeError):
        return np.nan

def check_age_match(age, faixa_etaria_str):
    if pd.isna(age) or not isinstance(faixa_etaria_str, str):
        return 0
    faixa_etaria_str_standardized = standardize_text(faixa_etaria_str)
    numbers = re.findall(r'\d+', faixa_etaria_str_standardized)
    if len(numbers) >= 2:
        try:
            min_age = int(numbers[0])
            max_age = int(numbers[1])
            return int(age >= min_age and age <= max_age)
        except ValueError:
            return 0
    return 0

def check_language_match(candidato_level, vaga_level):
    candidato_level = standardize_text(candidato_level)
    vaga_level = standardize_text(vaga_level)
    levels = {"nenhum": 0, "basico": 1, "intermediario": 2, "avancado": 3, "fluente": 4}
    cand_val = levels.get(candidato_level, 0)
    vaga_val = levels.get(vaga_level, 0)
    if vaga_val == 0:
        return 1
    return int(cand_val >= vaga_val)

def count_certifications(cert_str):
    if not isinstance(cert_str, str) or not cert_str.strip():
        return 0
    certs = [standardize_text(c) for c in cert_str.split(",") if standardize_text(c)]
    return len(certs)

# --- Função de Extração de Localização do Candidato para a previsão (parte do seu código 6.1) ---
def extract_candidate_location(row, location_type):
    location_str = get_nested_value(row, "infos_basicas.local", "")
    if isinstance(location_str, str) and location_str.strip():
        standardized_parts = [p for p in re.split(r'[\s,\-]+', standardize_text(location_str)) if p]

        if location_type == 'pais':
            if 'brasil' in standardized_parts: return 'brasil'
            return 'brasil' # Padrão para Brasil se não especificado ou se for Brasil

        elif location_type == 'estado':
            if 'rio' in standardized_parts and 'de' in standardized_parts and 'janeiro' in standardized_parts:
                return 'rio de janeiro'
            if 'sao' in standardized_parts and 'paulo' in standardized_parts:
                return 'sao paulo'
            if len(standardized_parts) >= 2 and standardized_parts[0] == standardized_parts[1] and len(standardized_parts[0]) > 2:
                return standardized_parts[0] # Ex: "Sao Paulo, SP" -> "sao" se for estado (simplificado)

            if len(standardized_parts[-1]) == 2 and standardized_parts[-1] in ["sp", "rj", "mg", "ba", "rs", "pr", "sc", "df", "go", "es", "ce", "pe", "pa", "am", "al", "ap", "pb", "pi", "rn", "se", "to", "ro", "rr", "ac", "ma", "ms", "mt"]:
                return standardized_parts[-1] # Siglas de estado

            if len(standardized_parts) > 1 and standardized_parts[1] not in ['de', 'da', 'do']:
                return standardized_parts[1] # Segundo termo, se não for preposição
            return "" # Não encontrou estado
        elif location_type == 'cidade':
            if standardized_parts:
                if 'rio' in standardized_parts and 'de' in standardized_parts and 'janeiro' in standardized_parts:
                    return 'rio de janeiro'
                if 'sao' in standardized_parts and 'paulo' in standardized_parts:
                    return 'sao paulo'
                return standardized_parts[0] # Primeiro termo como cidade
            return "" # Não encontrou cidade

    # Tentar extrair do endereço se infos_basicas.local falhar
    address_str = standardize_text(get_nested_value(row, "informacoes_pessoais.endereco", ""))
    if address_str:
        if location_type == 'cidade':
            return address_str # Pode ser uma cidade diretamente
        elif location_type == 'estado':
            if 'sao paulo' in address_str: return 'sao paulo'
            if 'rio de janeiro' in address_str: return 'rio de janeiro'
            # Poderíamos adicionar mais lógica para estados aqui, mas seria complexo sem uma base de dados de estados/cidades
            return ""
        elif location_type == 'pais':
            return 'brasil'
    return "" # Default se nada for encontrado

print("\n### 11. Exemplo de previsão com novos dados")

# --- Carregar o modelo treinado ---
modelo_filename = "modelo_compatibilidade.joblib"
try:
    rf = joblib.load(modelo_filename)
    print(f"Modelo carregado com sucesso de '{modelo_filename}'.")
except FileNotFoundError:
    print(f"Erro: Modelo '{modelo_filename}' não encontrado. Certifique-se de que a Parte 10 foi executada.")
    exit() # Encerrar se o modelo não puder ser carregado
except Exception as e:
    print(f"Erro ao carregar o modelo: {e}")
    exit()

# Verificar se X_train.columns está disponível (do treinamento)
if 'X_train' not in locals():
    print("Erro: 'X_train' não está disponível no ambiente. O DataFrame X_train (usado para treinar o modelo) é necessário para garantir que as colunas do novo_X correspondam.")
    print("Por favor, execute as Partes 7 e 8 para definir 'X_train' ou carregue-o se salvo.")
    exit()

# Exemplo de novo candidato e vaga (dados de entrada)
novo_candidato_vaga_data = {
    "id_vaga": "nova_vaga_id_001",
    "id_candidato": "novo_candidato_id_001",
    "situacao_candidado": "", # Não é usada como feature
    "prospects": [],
    "infos_basicas": {
        "telefone_recado": "", "telefone": "", "objetivo_profissional": "",
        "data_criacao": "", "inserido_por": "", "email": "",
        "local": "Belo Horizonte, Minas Gerais",
        "sabendo_de_nos_por": "", "data_atualizacao": "",
        "codigo_profissional": "", "nome": "João Silva"
    },
    "informacoes_pessoais": {
        "data_aceite": "", "nome": "João Silva", "cpf": "", "fonte_indicacao": "",
        "email": "", "email_secundario": "", "data_nascimento": "15-03-1990", # Exemplo: 35 anos em 2025
        "telefone_celular": "", "telefone_recado": "", "sexo": "Masculino",
        "estado_civil": "Solteiro", "pcd": "Não",
        "endereco": "Rua Exemplo, 123, Centro, Belo Horizonte, MG, Brasil",
        "skype": "", "url_linkedin": "linkedin.com/joao", "facebook": ""
    },
    "informacoes_profissionais": {
        "titulo_profissional": "Cientista de Dados", "area_atuacao": "Tecnologia", "conhecimentos_tecnicos": "Python, R, SQL, Machine Learning, Deep Learning, Spark",
        "certificacoes": "Google Cloud Professional Data Engineer, AWS Certified Machine Learning Specialty",
        "outras_certificacoes": "Curso XYZ de NLP", "remuneracao": "10000", "nivel_profissional": "Pleno"
    },
    "formacao_e_idiomas": {
        "nivel_academico": "Mestrado", # Exemplo
        "nivel_ingles": "Fluente", # Exemplo
        "nivel_espanhol": "Básico", "outro_idioma": ""
    },
    "cargo_atual": {
        "nome_empresa": "ABC Tech",
        "cargo": "Cientista de Dados Pleno",
        "data_inicio": "01/01/2020",
        "data_fim": "atual"
    },
    "cv_pt": "Experiência em projetos de Machine Learning com Python e SQL. Desenvolvi modelos de previsão e otimização. Trabalhei de 01/01/2020 a 28/06/2025 na Empresa ABC Tech como Cientista de Dados. Fui responsável pela análise de dados e implementação de algoritmos.",
    "cv_en": "Experience in Machine Learning projects with Python and SQL. Developed predictive and optimization models. Worked from 01/01/2020 to 28/06/2025 at ABC Tech as a Data Scientist. Responsible for data analysis and algorithm implementation."
    ,
    "informacoes_basicas": {
        "data_requicisao": "2025-06-01", "limite_espe": ""
    },
    "perfil_vaga": {
        "pais": "Brasil", "estado": "Minas Gerais", "cidade": "Belo Horizonte",
        "bairro": "", "regiao": "", "local_trabalho": "Híbrido",
        "vaga_especifica_para_pcd": "Não",
        "faixa_etaria": "De: 25 Até: 38",
        "horario_trabalho": "Comercial",
        "nivel profissional": "Pleno", # Exemplo
        "nivel_academico": "Ensino Superior Completo",
        "nivel_ingles": "Avançado", # Exemplo
        "nivel_espanhol": "Nenhum", "outro_idioma": "",
        "areas_atuacao": "Data Science, Inteligência Artificial",
        "principais_atividades": "Desenvolvimento e manutenção de modelos de Machine Learning em Python. Análise exploratória de dados. Colaboração com equipes de produto.",
        "competencia_tecnicas_e_comportamentais": "Python, SQL, ML, Comunicação, Resolução de problemas.",
        "demais_observacoes": "", "viagens_requeridas": "Não", "equipamentos_necessarios": ""
    },
    "beneficios": {
        "valor_venda": "", "valor_compra_1": "",
    }
}

# Criar um DataFrame a partir do novo exemplo
# O DataFrame de entrada deve ter o mesmo formato (colunas JSON aninhadas) do DataFrame original
novo_df = pd.DataFrame([novo_candidato_vaga_data])

print("\nAplicando as mesmas transformações de engenharia de atributos ao novo exemplo...")

# --- 1. Extração de Localização ---
# Use a função extract_candidate_location que já foi definida.
novo_df["pais_candidato"] = novo_df.apply(lambda row: extract_candidate_location(row, "pais"), axis=1)
novo_df["estado_candidato"] = novo_df.apply(lambda row: extract_candidate_location(row, "estado"), axis=1)
novo_df["cidade_candidato"] = novo_df.apply(lambda row: extract_candidate_location(row, "cidade"), axis=1)

novo_df["pais_vaga"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.pais", "")), axis=1)
novo_df["estado_vaga"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.estado", "")), axis=1)
novo_df["cidade_vaga"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.cidade", "")), axis=1)
novo_df["local_trabalho_vaga"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.local_trabalho", "")), axis=1)

novo_df["match_pais"] = (novo_df["pais_candidato"] == novo_df["pais_vaga"]).astype(int)
novo_df["match_estado"] = (novo_df["estado_candidato"] == novo_df["estado_vaga"]).astype(int)
novo_df["match_cidade"] = (novo_df["cidade_candidato"] == novo_df["cidade_vaga"]).astype(int)

# --- 2. Validação PCD ---
novo_df["pcd_candidato"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "informacoes_pessoais.pcd", "")), axis=1)
novo_df["vaga_pcd"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.vaga_especifica_para_pcd", "")), axis=1)
novo_df["match_pcd"] = ((novo_df["vaga_pcd"] == "sim") & (novo_df["pcd_candidato"] == "sim")).astype(int)

# --- 3. Faixa Etária ---
novo_df["idade_candidato"] = novo_df.apply(lambda row: calculate_age(get_nested_value(row, "informacoes_pessoais.data_nascimento", "")), axis=1)
novo_df["faixa_etaria_vaga"] = novo_df.apply(lambda row: get_nested_value(row, "perfil_vaga.faixa_etaria", ""), axis=1)
novo_df["match_faixa_etaria"] = novo_df.apply(lambda row: check_age_match(row["idade_candidato"], row["faixa_etaria_vaga"]), axis=1)

# --- 4. Nível Acadêmico ---
novo_df["nivel_academico_candidato"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "formacao_e_idiomas.nivel_academico", "")), axis=1)
novo_df["nivel_academico_vaga"] = novo_df.apply(lambda row: standardize_text(get_nested_value(row, "perfil_vaga.nivel_academico", "")), axis=1)
novo_df["match_nivel_academico"] = (novo_df["nivel_academico_candidato"] == novo_df["nivel_academico_vaga"]).astype(int)

# --- 5. Idiomas ---
novo_df["nivel_ingles_candidato"] = novo_df.apply(lambda row: get_nested_value(row, "formacao_e_idiomas.nivel_ingles", ""), axis=1)
novo_df["nivel_espanhol_candidato"] = novo_df.apply(lambda row: get_nested_value(row, "formacao_e_idiomas.nivel_espanhol", ""), axis=1)
novo_df["nivel_ingles_vaga"] = novo_df.apply(lambda row: get_nested_value(row, "perfil_vaga.nivel_ingles", ""), axis=1)
novo_df["nivel_espanhol_vaga"] = novo_df.apply(lambda row: get_nested_value(row, "perfil_vaga.nivel_espanhol", ""), axis=1)
novo_df["match_ingles"] = novo_df.apply(lambda row: check_language_match(row["nivel_ingles_candidato"], row["nivel_ingles_vaga"]), axis=1)
novo_df["match_espanhol"] = novo_df.apply(lambda row: check_language_match(row["nivel_espanhol_candidato"], row["nivel_espanhol_vaga"]), axis=1)

# --- 6. Certificações (Contagem) ---
novo_df["num_certificacoes_candidato"] = novo_df.apply(lambda row: count_certifications(get_nested_value(row, "informacoes_profissionais.certificacoes", "")), axis=1)
novo_df["num_outras_certificacoes_candidato"] = novo_df.apply(lambda row: count_certifications(get_nested_value(row, "informacoes_profissionais.outras_certificacoes", "")), axis=1)

# --- 7. Experiência ---
novo_df["experiencia_candidato_anos"] = novo_df.apply(lambda row: extract_experience_years(get_nested_value(row, "cv_pt", "")), axis=1)
novo_df["nivel_profissional_vaga"] = novo_df.apply(lambda row: get_nested_value(row, "perfil_vaga.nivel profissional", "") if 'perfil_vaga' in row and isinstance(row['perfil_vaga'], dict) else "", axis=1)
novo_df["experiencia_vaga_anos"] = novo_df["nivel_profissional_vaga"].apply(map_nivel_profissional_to_experience)
novo_df["dif_experiencia"] = novo_df["experiencia_candidato_anos"] - novo_df["experiencia_vaga_anos"]
novo_df["atende_experiencia"] = (novo_df["experiencia_candidato_anos"] >= novo_df["experiencia_vaga_anos"]).astype(int)

# --- 8. Habilidades/Atividades (CORRIGIDO: replicando a lógica da 6.2) ---
# Usamos a lógica de tokenização e intersecção diretamente aqui.
novo_df["num_habilidades_em_comum"] = novo_df.apply(
    lambda row: len(
        set(clean_and_tokenize(str(get_nested_value(row, "perfil_vaga.principais_atividades", "")) + " " +
                               str(get_nested_value(row, "perfil_vaga.competencia_tecnicas_e_comportamentais", "")), min_word_len=5))
        .intersection(
            set(clean_and_tokenize(str(row.get("cv_pt", "")) + " " + str(row.get("cv_en", "")), min_word_len=5))
        )
    ),
    axis=1
)
# A coluna 'match_habilidades' foi criada com base na condição >=4, replicando o que foi feito no treinamento.
novo_df["match_habilidades"] = (novo_df["num_habilidades_em_comum"] >= 4).astype(int)


print("\nRemovendo colunas originais e não-features para o DataFrame de previsão...")
# Remover colunas que não são features do modelo (mesma lista da Parte 7)
colunas_para_remover_novo_X = [
    "id_vaga", "id_candidato", "situacao_candidado", "prospects",
    "infos_basicas", "informacoes_pessoais", "informacoes_profissionais", "formacao_e_idiomas", "cargo_atual",
    "cv_pt", "cv_en", "informacoes_basicas", "perfil_vaga", "beneficios",
    "pais_candidato", "estado_candidato", "cidade_candidato",
    "pais_vaga", "estado_vaga", "cidade_vaga", "local_trabalho_vaga",
    "pcd_candidato", "vaga_pcd",
    "idade_candidato", "faixa_etaria_vaga",
    "nivel_academico_candidato", "nivel_academico_vaga",
    "nivel_ingles_candidato", "nivel_ingles_vaga",
    "nivel_espanhol_candidato", "nivel_espanhol_vaga",
    "nivel_profissional_vaga",
    # Removemos "vaga_tokens" e "candidato_tokens" aqui, pois não são features finais, mas intermediárias.
    # Se você adicionou essas colunas no df original e as removeu em X_train, mantenha-as aqui.
]

# Filtrar apenas as colunas que realmente existem no DataFrame antes de tentar removê-las
colunas_existentes_para_remover_novo_X = [col for col in colunas_para_remover_novo_X if col in novo_df.columns]
novo_X = novo_df.drop(columns=colunas_existentes_para_remover_novo_X, errors="ignore")

# --- Tratamento de colunas numéricas e NaNs para o novo_X (replicando a Parte 7) ---
# Identificar colunas não numéricas remanescentes no novo_X
non_numeric_cols_novo_X = novo_X.select_dtypes(include=['object', 'category']).columns

if not non_numeric_cols_novo_X.empty:
    for col in non_numeric_cols_novo_X:
        # Reaplicar OHE ou remoção, baseado na lógica da Parte 7
        # Para OHE, é crucial que as categorias sejam as mesmas do X_train.
        # Para um único exemplo, isso pode ser um desafio se uma categoria nova aparecer.
        # A forma mais robusta seria usar um ColumnTransformer pré-treinado/fitado no X_train.
        # Para este exemplo simples, vamos tentar com get_dummies e depois alinhar.
        if novo_X[col].nunique() < 50: # Limite arbitrário, como na Parte 7
            dummies = pd.get_dummies(novo_X[col], prefix=col, dummy_na=False, drop_first=True)
            novo_X = pd.concat([novo_X, dummies], axis=1)
            novo_X = novo_X.drop(columns=[col])
        else:
            novo_X = novo_X.drop(columns=[col])

# Preencher quaisquer NaNs restantes em novo_X com 0 (ou média do X_train, se souber)
# Para um único exemplo, 0 é mais seguro do que a média se a média do X_train não for conhecida.
# Se X_train foi preenchido com a média, o ideal seria usar a média *daquela coluna em X_train*.
for col in novo_X.select_dtypes(include=np.number).columns:
    if novo_X[col].isnull().any():
        novo_X[col] = novo_X[col].fillna(0) # Preenche com 0, ou use a média do X_train para essa coluna.

# Garantir que as colunas de novo_X correspondam EXATAMENTE às colunas de X_train
# Preencher com 0 para colunas que estão em X_train mas não no novo_X (novas categorias de OHE, por exemplo)
missing_cols_in_new = set(X_train.columns) - set(novo_X.columns)
for c in missing_cols_in_new:
    novo_X[c] = 0

# Remover colunas que estão no novo_X mas não em X_train (categorias que não existiam no treinamento)
extra_cols_in_new = set(novo_X.columns) - set(X_train.columns)
for c in extra_cols_in_new:
    novo_X = novo_X.drop(columns=[c])

# Garantir a ordem das colunas para a previsão (CRUCIAL!)
novo_X = novo_X[X_train.columns]

print(f"\nDataFrame 'novo_X' preparado para previsão com {novo_X.shape[0]} linhas e {novo_X.shape[1]} colunas.")
print("Primeiras 5 colunas de 'novo_X':", novo_X.columns[:5].tolist())
print("Últimas 5 colunas de 'novo_X':", novo_X.columns[-5:].tolist())


# Fazer previsão
if not novo_X.empty:
    if 'rf' in locals() and hasattr(rf, 'predict'):
        probabilidade = rf.predict_proba(novo_X)[:,1][0]
        previsao = rf.predict(novo_X)[0]
        print(f"\nProbabilidade de ser contratado (sucesso): {probabilidade:.2f}")
        print("Compatível com a vaga:" , "Sim" if previsao == 1 else "Não")
    else:
        print("Erro: O modelo 'rf' não está carregado ou não tem o método 'predict'.")
else:
    print("DataFrame 'novo_X' está vazio após o pré-processamento. Não é possível fazer a previsão.")

print("\nEtapa de previsão com novos dados concluída na Parte 11.")